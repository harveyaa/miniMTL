{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harveyaa/miniconda3/envs/MTL/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from miniMTL.datasets import *\n",
    "from miniMTL.models import *\n",
    "from miniMTL.util import *\n",
    "from miniMTL.training import *\n",
    "from miniMTL.hps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder3(nn.Module):\n",
    "    def __init__(self,dim=58,width=10):\n",
    "        super().__init__()\n",
    "        # in_channels, out_channels\n",
    "        self.fc1 = nn.Linear(dim, 256)\n",
    "        self.batch1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 16)\n",
    "        self.batch2 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.leaky(self.fc1(x)))\n",
    "        x = self.batch1(x)\n",
    "        x = self.dropout(self.leaky(self.fc2(x)))\n",
    "        x = self.batch2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class head3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc3 = nn.Linear(16,2)\n",
    "        #self.batch3 = nn.BatchNorm1d(16)\n",
    "        #self.fc4 = nn.Linear(width,2)\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.leaky(self.fc3(x)))\n",
    "        #x = self.batch3(x)\n",
    "        #x = self.dropout(F.relu(self.fc4(x)))\n",
    "        #x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pheno = '/home/harveyaa/Documents/fMRI/data/ukbb_9cohorts/pheno_01-12-21.csv'\n",
    "p_ids = '/home/harveyaa/Documents/masters/neuropsych_mtl/datasets/cv_folds/hybrid'\n",
    "p_conn = '/home/harveyaa/Documents/fMRI/data/ukbb_9cohorts/connectomes/'\n",
    "\n",
    "cases = ['SZ',\n",
    "        #'BIP',\n",
    "        #'ASD',\n",
    "        'DEL22q11_2',\n",
    "        #'DEL16p11_2',\n",
    "        #'DUP16p11_2',\n",
    "        #'DUP22q11_2',\n",
    "        #'DEL1q21_1',\n",
    "        #'DUP1q21_1'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate 22q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pd.read_csv('/home/harveyaa/Documents/fMRI/data/ukbb_9cohorts/connectomes_01-12-21.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22q = pd.read_csv(os.path.join(p_ids,'DEL22q11_2.csv'),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = conn[conn.index.isin(df_22q.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 2080)\n",
      "(28, 2080)\n",
      "(58, 1)\n",
      "(28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harveyaa/miniconda3/envs/MTL/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6071428571428571"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(C=100)\n",
    "fold = 4\n",
    "\n",
    "train_idx = df_22q[df_22q[f'fold_{fold}'] == 0].index\n",
    "test_idx = df_22q[df_22q[f'fold_{fold}'] == 1].index\n",
    "\n",
    "X_train = conn[conn.index.isin(train_idx)].values\n",
    "X_test = conn[conn.index.isin(test_idx)].values\n",
    "y_train = df_22q[df_22q.index.isin(train_idx)]['DEL22q11_2'].values.reshape(-1,1)\n",
    "y_test = df_22q[df_22q.index.isin(test_idx)]['DEL22q11_2'].values.reshape(-1,1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harveyaa/miniconda3/envs/MTL/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (7,8,12,13,14,19,20,24,27,31,42,43,44,49,51,59,60,62,64,65,68,101,121,163) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pheno = pd.read_csv(p_pheno,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ['AGE',\n",
    "            'SEX',\n",
    "            'SITE',\n",
    "            'mean_conn',\n",
    "            'FD_scrubbed']\n",
    "case = 'DEL22q11_2'\n",
    "\n",
    "# PLOT TEST SET\n",
    "#fig, ax = plt.subplots(len(conf),5,figsize=(15,12))\n",
    "#for i,c in enumerate(conf):\n",
    "#        for fold in range(5):\n",
    "#                #ids = pd.read_csv(os.path.join(temp_dir.name,f\"{case}_test_set_{fold}.txt\"),header=None)\n",
    "#                ids = df_22q[df_22q[f'fold_{fold}']==1].index\n",
    "#                \n",
    "#                sns.histplot(x=c,data=pheno[pheno.index.isin(ids)],hue=case,bins=25,ax=ax[i,fold])\n",
    "#                if i == 0:\n",
    "#                        ax[i,fold].set_title(f'fold {fold}')\n",
    "#                if fold == 0:\n",
    "#                        ax[i,fold].set_xlabel('')\n",
    "#                        ax[i,fold].set_ylabel(c)\n",
    "#                else:\n",
    "#                        ax[i,fold].set_xlabel('')\n",
    "#                        ax[i,fold].set_ylabel('')\n",
    "#                        ax[i,fold].set_yticklabels([])\n",
    "#plt.tight_layout()\n",
    "#plt.subplots_adjust(wspace=0.1,hspace=0.2)\n",
    "#plt.savefig(os.path.join(args.p_out,f\"{case}_test.png\"),dpi=300\n",
    "\n",
    "# PLOT TRAIN SET\n",
    "#fig, ax = plt.subplots(len(conf),5,figsize=(15,12))\n",
    "#for i,c in enumerate(conf):\n",
    "#        for fold in range(5):\n",
    "#                ids_train = ids = df_22q[df_22q[f'fold_{fold}']==0].index\n",
    "#                \n",
    "#                sns.histplot(x=c,data=pheno[pheno.index.isin(ids_train)],hue=case,bins=25,ax=ax[i,fold])\n",
    "#                if i == 0:\n",
    "#                        ax[i,fold].set_title(f'fold {fold}')\n",
    "#                if fold == 0:\n",
    "#                        ax[i,fold].set_xlabel('')\n",
    "#                        ax[i,fold].set_ylabel(c)\n",
    "#                else:\n",
    "#                        ax[i,fold].set_xlabel('')\n",
    "#                        ax[i,fold].set_ylabel('')\n",
    "#                        ax[i,fold].set_yticklabels([])\n",
    "#plt.tight_layout()\n",
    "#plt.subplots_adjust(wspace=0.1,hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "SZ\n",
      "DEL22q11_2\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "print('Creating datasets...')\n",
    "data = []\n",
    "for case in cases:\n",
    "    print(case)\n",
    "    data.append(balancedCaseControlDataset(case,p_ids,p_conn,format=0))\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BALANCED TEST SETS\n",
    "\n",
    "batch_size=4\n",
    "#head=0\n",
    "#encoder=0\n",
    "fold=0\n",
    "width=100\n",
    "\n",
    "loss_fns = {}\n",
    "trainloaders = {}\n",
    "testloaders = {}\n",
    "decoders = {}\n",
    "for d, case in zip(data,cases):\n",
    "    train_idx, test_idx = d.split_data(fold)\n",
    "    train_d = Subset(d,train_idx)\n",
    "    test_d = Subset(d,test_idx)\n",
    "    trainloaders[case] = DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "    testloaders[case] = DataLoader(test_d, batch_size=batch_size, shuffle=True)\n",
    "    loss_fns[case] = nn.CrossEntropyLoss()\n",
    "    #decoders[case] = eval(f'head{head}().double()')\n",
    "    decoders[case] = head3().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM TEST SETS\n",
    "\n",
    "#batch_size=4\n",
    "#head=0\n",
    "#encoder=0\n",
    "#\n",
    "## Split data & create loaders & loss fns\n",
    "#loss_fns = {}\n",
    "#trainloaders = {}\n",
    "#testloaders = {}\n",
    "#decoders = {}\n",
    "#for d, case in zip(data,cases):\n",
    "#    train_d, test_d = split_data(d,seed=888)\n",
    "#    trainloaders[case] = DataLoader(train_d, batch_size=batch_size, shuffle=True)\n",
    "#    testloaders[case] = DataLoader(test_d, batch_size=batch_size, shuffle=True)\n",
    "#    loss_fns[case] = nn.CrossEntropyLoss()\n",
    "#    decoders[case] = eval(f'head{head}().double()')\n",
    "#    #decoders[case] = head3().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized HPSModel using: cpu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = HPSModel(#eval(f'encoder{encoder}().double()'),\n",
    "                encoder3(dim=2080).double(),\n",
    "                decoders,\n",
    "                loss_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_pre = []\n",
    "#for p in model.parameters():\n",
    "#    params_pre.append(torch.tensor(p[0]))\n",
    "#params_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harveyaa/Documents/masters/neuropsych_mtl/tmp\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/home/harveyaa/Documents/masters/neuropsych_mtl/tmp'\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=100\n",
    "lr = 0.001\n",
    "\n",
    "# Create optimizer & trainer\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=lr)\n",
    "trainer = Trainer(optimizer,num_epochs=num_epochs,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 142/142 [00:11<00:00, 12.51it/s]\n",
      "Epoch 1: 100%|██████████| 142/142 [00:11<00:00, 12.75it/s]\n",
      "Epoch 2: 100%|██████████| 142/142 [00:11<00:00, 12.77it/s]\n",
      "Epoch 3: 100%|██████████| 142/142 [00:11<00:00, 12.88it/s]\n",
      "Epoch 4: 100%|██████████| 142/142 [00:10<00:00, 13.07it/s]\n",
      "Epoch 5: 100%|██████████| 142/142 [00:11<00:00, 12.43it/s]\n",
      "Epoch 6: 100%|██████████| 142/142 [00:11<00:00, 12.54it/s]\n",
      "Epoch 7: 100%|██████████| 142/142 [00:17<00:00,  8.25it/s]\n",
      "Epoch 8: 100%|██████████| 142/142 [00:18<00:00,  7.76it/s]\n",
      "Epoch 9: 100%|██████████| 142/142 [00:12<00:00, 11.31it/s]\n",
      "Epoch 10: 100%|██████████| 142/142 [00:12<00:00, 11.34it/s]\n",
      "Epoch 11: 100%|██████████| 142/142 [00:12<00:00, 10.97it/s]\n",
      "Epoch 12: 100%|██████████| 142/142 [00:14<00:00, 10.01it/s]\n",
      "Epoch 13: 100%|██████████| 142/142 [00:12<00:00, 10.95it/s]\n",
      "Epoch 14: 100%|██████████| 142/142 [00:12<00:00, 11.47it/s]\n",
      "Epoch 15: 100%|██████████| 142/142 [00:12<00:00, 11.27it/s]\n",
      "Epoch 16: 100%|██████████| 142/142 [00:12<00:00, 11.47it/s]\n",
      "Epoch 17: 100%|██████████| 142/142 [00:12<00:00, 11.36it/s]\n",
      "Epoch 18: 100%|██████████| 142/142 [00:12<00:00, 11.57it/s]\n",
      "Epoch 19: 100%|██████████| 142/142 [00:12<00:00, 11.22it/s]\n",
      "Epoch 20: 100%|██████████| 142/142 [00:12<00:00, 10.96it/s]\n",
      "Epoch 21: 100%|██████████| 142/142 [00:12<00:00, 11.43it/s]\n",
      "Epoch 22: 100%|██████████| 142/142 [00:12<00:00, 11.54it/s]\n",
      "Epoch 23: 100%|██████████| 142/142 [00:12<00:00, 11.49it/s]\n",
      "Epoch 24: 100%|██████████| 142/142 [00:12<00:00, 11.29it/s]\n",
      "Epoch 25: 100%|██████████| 142/142 [00:12<00:00, 11.43it/s]\n",
      "Epoch 26: 100%|██████████| 142/142 [00:12<00:00, 11.25it/s]\n",
      "Epoch 27: 100%|██████████| 142/142 [00:12<00:00, 11.19it/s]\n",
      "Epoch 28: 100%|██████████| 142/142 [00:12<00:00, 11.24it/s]\n",
      "Epoch 29: 100%|██████████| 142/142 [00:12<00:00, 11.20it/s]\n",
      "Epoch 30: 100%|██████████| 142/142 [00:12<00:00, 11.23it/s]\n",
      "Epoch 31: 100%|██████████| 142/142 [00:12<00:00, 11.18it/s]\n",
      "Epoch 32: 100%|██████████| 142/142 [00:12<00:00, 11.33it/s]\n",
      "Epoch 33: 100%|██████████| 142/142 [00:12<00:00, 11.32it/s]\n",
      "Epoch 34: 100%|██████████| 142/142 [00:12<00:00, 11.31it/s]\n",
      "Epoch 35: 100%|██████████| 142/142 [00:12<00:00, 11.25it/s]\n",
      "Epoch 36: 100%|██████████| 142/142 [00:12<00:00, 11.16it/s]\n",
      "Epoch 37: 100%|██████████| 142/142 [00:12<00:00, 11.19it/s]\n",
      "Epoch 38: 100%|██████████| 142/142 [00:12<00:00, 11.36it/s]\n",
      "Epoch 39: 100%|██████████| 142/142 [00:12<00:00, 11.13it/s]\n",
      "Epoch 40: 100%|██████████| 142/142 [00:12<00:00, 11.03it/s]\n",
      "Epoch 41: 100%|██████████| 142/142 [00:13<00:00, 10.61it/s]\n",
      "Epoch 42: 100%|██████████| 142/142 [00:12<00:00, 11.39it/s]\n",
      "Epoch 43: 100%|██████████| 142/142 [00:13<00:00, 10.55it/s]\n",
      "Epoch 44: 100%|██████████| 142/142 [00:13<00:00, 10.84it/s]\n",
      "Epoch 45: 100%|██████████| 142/142 [00:12<00:00, 11.09it/s]\n",
      "Epoch 46: 100%|██████████| 142/142 [00:12<00:00, 11.13it/s]\n",
      "Epoch 47: 100%|██████████| 142/142 [00:12<00:00, 11.35it/s]\n",
      "Epoch 48: 100%|██████████| 142/142 [00:12<00:00, 11.12it/s]\n",
      "Epoch 49: 100%|██████████| 142/142 [00:12<00:00, 11.23it/s]\n",
      "Epoch 50: 100%|██████████| 142/142 [00:12<00:00, 11.01it/s]\n",
      "Epoch 51: 100%|██████████| 142/142 [00:12<00:00, 11.21it/s]\n",
      "Epoch 52: 100%|██████████| 142/142 [00:12<00:00, 11.03it/s]\n",
      "Epoch 53: 100%|██████████| 142/142 [00:12<00:00, 11.21it/s]\n",
      "Epoch 54: 100%|██████████| 142/142 [00:13<00:00, 10.89it/s]\n",
      "Epoch 55: 100%|██████████| 142/142 [00:13<00:00, 10.76it/s]\n",
      "Epoch 56: 100%|██████████| 142/142 [00:12<00:00, 11.00it/s]\n",
      "Epoch 57: 100%|██████████| 142/142 [00:12<00:00, 11.08it/s]\n",
      "Epoch 58: 100%|██████████| 142/142 [00:12<00:00, 11.30it/s]\n",
      "Epoch 59: 100%|██████████| 142/142 [00:12<00:00, 11.05it/s]\n",
      "Epoch 60: 100%|██████████| 142/142 [00:12<00:00, 11.22it/s]\n",
      "Epoch 61: 100%|██████████| 142/142 [00:12<00:00, 11.13it/s]\n",
      "Epoch 62: 100%|██████████| 142/142 [00:12<00:00, 11.33it/s]\n",
      "Epoch 63: 100%|██████████| 142/142 [00:12<00:00, 11.06it/s]\n",
      "Epoch 64: 100%|██████████| 142/142 [00:13<00:00, 10.71it/s]\n",
      "Epoch 65: 100%|██████████| 142/142 [00:12<00:00, 11.11it/s]\n",
      "Epoch 66: 100%|██████████| 142/142 [00:12<00:00, 11.15it/s]\n",
      "Epoch 67: 100%|██████████| 142/142 [00:13<00:00, 10.23it/s]\n",
      "Epoch 68: 100%|██████████| 142/142 [00:12<00:00, 11.12it/s]\n",
      "Epoch 69: 100%|██████████| 142/142 [00:12<00:00, 11.24it/s]\n",
      "Epoch 70: 100%|██████████| 142/142 [00:13<00:00, 10.68it/s]\n",
      "Epoch 71: 100%|██████████| 142/142 [00:12<00:00, 11.21it/s]\n",
      "Epoch 72: 100%|██████████| 142/142 [00:14<00:00,  9.97it/s]\n",
      "Epoch 73: 100%|██████████| 142/142 [00:13<00:00, 10.42it/s]\n",
      "Epoch 74: 100%|██████████| 142/142 [00:12<00:00, 11.34it/s]\n",
      "Epoch 75: 100%|██████████| 142/142 [00:12<00:00, 11.09it/s]\n",
      "Epoch 76: 100%|██████████| 142/142 [00:12<00:00, 11.26it/s]\n",
      "Epoch 77: 100%|██████████| 142/142 [00:12<00:00, 11.09it/s]\n",
      "Epoch 78: 100%|██████████| 142/142 [00:12<00:00, 11.26it/s]\n",
      "Epoch 79: 100%|██████████| 142/142 [00:12<00:00, 11.02it/s]\n",
      "Epoch 80: 100%|██████████| 142/142 [00:12<00:00, 11.26it/s]\n",
      "Epoch 81: 100%|██████████| 142/142 [00:12<00:00, 11.17it/s]\n",
      "Epoch 82: 100%|██████████| 142/142 [00:12<00:00, 11.26it/s]\n",
      "Epoch 83: 100%|██████████| 142/142 [00:12<00:00, 11.23it/s]\n",
      "Epoch 84: 100%|██████████| 142/142 [00:12<00:00, 11.20it/s]\n",
      "Epoch 85: 100%|██████████| 142/142 [00:13<00:00, 10.41it/s]\n",
      "Epoch 86: 100%|██████████| 142/142 [00:12<00:00, 11.16it/s]\n",
      "Epoch 87: 100%|██████████| 142/142 [00:12<00:00, 11.24it/s]\n",
      "Epoch 88: 100%|██████████| 142/142 [00:12<00:00, 11.18it/s]\n",
      "Epoch 89: 100%|██████████| 142/142 [00:12<00:00, 11.31it/s]\n",
      "Epoch 90: 100%|██████████| 142/142 [00:13<00:00, 10.53it/s]\n",
      "Epoch 91: 100%|██████████| 142/142 [00:12<00:00, 11.31it/s]\n",
      "Epoch 92: 100%|██████████| 142/142 [00:12<00:00, 11.12it/s]\n",
      "Epoch 93: 100%|██████████| 142/142 [00:12<00:00, 11.37it/s]\n",
      "Epoch 94: 100%|██████████| 142/142 [00:12<00:00, 11.00it/s]\n",
      "Epoch 95: 100%|██████████| 142/142 [00:12<00:00, 11.31it/s]\n",
      "Epoch 96: 100%|██████████| 142/142 [00:12<00:00, 11.17it/s]\n",
      "Epoch 97: 100%|██████████| 142/142 [00:13<00:00, 10.49it/s]\n",
      "Epoch 98: 100%|██████████| 142/142 [00:13<00:00, 10.66it/s]\n",
      "Epoch 99: 100%|██████████| 142/142 [00:12<00:00, 10.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.fit(model,trainloaders,testloaders,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_post = []\n",
    "#for p in model.parameters():\n",
    "#    params_post.append(p[0])\n",
    "#params_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "torch.Size([2080])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n",
      "0\n",
      "torch.Size([256])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n",
      "0\n",
      "torch.Size([10])\n",
      "\n",
      "0\n",
      "torch.Size([])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(params_pre))\n",
    "for i in range(len(params_pre)):\n",
    "    print((params_pre[i]==params_post[i]).sum().item())\n",
    "    print(params_pre[i].size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SZ\n",
      "Accuracy:  51.587301587301596\n",
      "Loss:  0.04392023539256849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BALANCED\n",
    "# SZ 51.59\n",
    "# BIP 50.0\n",
    "# ASD 47.3\n",
    "\n",
    "# RANDOM\n",
    "# SZ 58.59\n",
    "# BIP 71.875\n",
    "# ASD 49.74\n",
    "\n",
    "# Evaluate at end\n",
    "metrics = model.score(testloaders)\n",
    "for key in metrics.keys():\n",
    "    print()\n",
    "    print(key)\n",
    "    print('Accuracy: ', metrics[key]['accuracy'])\n",
    "    print('Loss: ', metrics[key]['loss'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0])\n",
      "\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0])\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, Y_dict in iter(trainloaders['SZ']):\n",
    "    pred = model.forward(X,['SZ'])['SZ']\n",
    "    print(pred.argmax(1))\n",
    "    print(Y_dict['SZ'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2205,  0.7734,  0.3376,  ...,  0.4517,  0.7568,  0.2864],\n",
       "        [ 0.3252,  0.7770,  0.4318,  ...,  0.5235,  0.4512,  0.2063],\n",
       "        [ 0.2663,  1.0461,  0.5479,  ...,  0.0115, -0.2313,  0.2665],\n",
       "        ...,\n",
       "        [ 0.2632,  0.6835,  0.2740,  ...,  0.7039,  0.3534,  0.2338],\n",
       "        [ 0.1444,  0.4784,  0.2277,  ...,  0.2569,  0.5483,  0.2190],\n",
       "        [ 0.2306,  0.6204,  0.2626,  ...,  0.4079,  0.7664,  0.2759]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y_dict = next(iter(trainloaders['SZ']))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-313.3458, -115.3427],\n",
       "        [-251.3480,  -92.2653],\n",
       "        [-128.5347,  -47.3491],\n",
       "        [-154.7057,  -56.9959],\n",
       "        [-277.0331, -102.1275],\n",
       "        [-267.5116,  -96.1504],\n",
       "        [-110.4147,  -40.1776],\n",
       "        [-194.1211,  -71.5230],\n",
       "        [-262.9967,  -96.4557],\n",
       "        [-258.9930,  -96.4075],\n",
       "        [-230.1875,  -85.4877],\n",
       "        [-307.7387, -112.7890],\n",
       "        [ -94.0132,  -34.1477],\n",
       "        [-272.7833, -100.4545],\n",
       "        [-265.4162,  -97.0490],\n",
       "        [-299.4902, -109.9600]], dtype=torch.float64,\n",
       "       grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = nn.Dropout(p=0.001)\n",
    "leaky = nn.LeakyReLU()\n",
    "\n",
    "#X_1 = dropout(F.relu(model.encoder.fc1(X)))\n",
    "#X_2 = dropout(F.relu(model.encoder.fc2(X_1)))\n",
    "#X_3 = dropout(F.relu(model.decoders['SZ'].fc3(X_2)))\n",
    "\n",
    "X_1 = leaky(model.encoder.fc1(X))\n",
    "X_2 = leaky(model.encoder.fc2(X_1))\n",
    "X_3 = leaky(model.decoders['SZ'].fc3(X_2))\n",
    "X_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9c9c5c044d2ea5dfa7d6bd44b9f426810f06c0ed392c2b436f27e01061f47c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('MTL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
